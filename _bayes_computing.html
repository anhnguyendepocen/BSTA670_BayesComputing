<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>BSTA 670:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Arman Oganisian   aoganisi@upenn.edu" />
    <meta name="date" content="2019-04-23" />
    <link rel="stylesheet" href="style/penn.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# BSTA 670:
## Bayesian Computation: MCMC Sampling, Integration, and Approximate Inference
### Arman Oganisian <br/> <a href="mailto:aoganisi@upenn.edu">aoganisi@upenn.edu</a>
### April 23, 2019

---






## Overview of Bayesian Inference
- Parameter vector `\(\theta \in \mathbb{R}^p\)` and data `\(D\)`.
- `\(\mathcal{L}(  \theta |  D) = p( D|  \theta)\)` with prior `\(p(\theta)\)` over parameters space.

$$
`\begin{align}
	p(\theta | D) &amp; = C\cdot p( D|  \theta) p( \theta) \\
					&amp; \propto p( D|  \theta) p( \theta) \\
\end{align}`
$$
- Inference engines:
  - Frequentist: **optimization methods** for maximizing `\(p( D| \theta)\)`.
  - Bayesian: **sampling methods** for drawing from `\(p( \theta | D)\)`.
    - Difficult since `\(C\)` unknown.

---
## Gibbs Sampler for Linear Regression

- Data `\(D = (y_i,  x_i)_{1:n}\)` and `\(\theta = (\beta, \phi)\)`, where `\(x_i, \beta \in \mathbb{R}^{p+1}\)`.
- `\(p( D |  \theta) = \prod_ip(y_i |  x_i,  \theta ) \stackrel{d}{=} \prod_iN(y_i\ ;\ x_i '  \beta, \phi)\)`.
- If we use joint prior `\(p(\theta) = p(\beta)p(\phi) = N_{p+1}( 0,  I)IG(\alpha, \lambda)\)`, then

--

  - `\(p(\beta | \phi, D) = N_{p+1}\Big( (I + \frac{1}{\phi} X' X )^{-1}( \frac{1}{\phi} X' y)  , (I + \frac{1}{\phi} X' X )^{-1} \Big)\)`.
  - `\(p(\phi | \beta, D) = IG(\alpha + n/2, \lambda + \frac{1}{2} ( y -  X \beta)'( y - X \beta) )\)`
	
--

- **Gibbs Sampling**: sample from these two conditionals in alternating fashion
  -  `\(\beta^{(t)} | \phi^{(t-1)} \sim N_{p+1} \Big( ( I + \frac{1}{ \phi^{(t-1)}}  X'  X )^{-1}( \frac{1}{  \phi^{(t-1)} }  X'  y)  , ( I + \frac{1}{ \phi^{(t-1)}}  X'  X )^{-1} \Big)\)`.
  - `\(\phi^{(t)} |  \beta^{(t)} \sim IG(\alpha + n/2, \lambda + \frac{1}{2} ( y -  X  \beta^{(t)})'( y -  X  \beta^{(t)} ) )\)`.
- Claim: The samples `\(\{\beta^{(t)}, \phi^{(t)} \}_{1:T}\)` converge to draws from the posterior `\(p(\beta, \phi |  D)\)`.


---
## Gibbs Sampling



```r
for( i in 2:iter){
  post_cov &lt;- solve(Imat + (1/phi[i-1]) * xtx)
  post_mean &lt;- post_cov %*% ((1/phi[i-1]) * t(X) %*% y)
  beta[, i]  &lt;- MASS::mvrnorm(1, post_mean , post_cov )
  
  post_rate &lt;- 100 + .5*sum((y - X %*% beta[, i, drop=F])^2)
  post_shape &lt;- 5 + n/2
  phi[i] &lt;- invgamma::rinvgamma(1, post_shape, rate = post_rate)
}
```

--

- We can plot the sequences or "chains":  `\(\{\beta^{(t)} \}_{1:T}\)` and `\(\{\phi^{(t)} \}_{1:T}\)`.
- These are the **Monte Carlo Markov Chains**.
	- **Monte Carlo**: each element of the chain is randomly drawn/simulated.
	- **Markov**: `\(\theta^{(t)}\)` only depends on the previous element `\(\theta^{(t-1)}\)`.

---
## Gibbs Sampling

&lt;img src="_bayes_computing_files/figure-html/gibbs_anim-.gif" style="display: block; margin: auto;" /&gt;

---
## MCMC - Checks and Limitations of Gibbs
- After sampling, must conduct visual and formal checks for
	- Convergence.
	- Autocorrelation. 
	- Sensitivity to initial values.
- Gibbs requires known conditional posteriors: `\(p(\beta | \phi, D)\)`, `\(p(\phi | \beta, D)\)`.
- In models without conjugacy, these are unknown - all we know is the form of `\(p(\theta | D)\)` up to a proportionality constant.

---
## Sampling for a Logistic Regression

- Data `\(D = (y_i, x_i)_{1:n}\)`, where `\(x_i \in \mathbb{R}^{p+1}\)` and `\(y_i \in \{0,1\}\)`.	
`$$p( D | \theta) \stackrel{d}{=} \prod_iBer\big( \ y_i \ ; \ expit( x_i ' \theta) \big)$$` 

`$$p(\theta) \stackrel{d}{=} N_{p+1}( 0, I)$$`

- Posterior is unknown:
$$p(\theta | D) \propto N_{p+1}( 0, I) \prod_iBer\big( \ y_i \ ; \ expit( x_i ' \theta) \big) $$
- Gibbs can't be used here.

---
## The Metropolis-Hastings Sampler
&lt;small&gt;
Along with initial value, `\(\theta^{(0)}\)`, MH algorithm requires two inputs:
- **Unnormalized target density**, `\(\tilde p(\theta | D)\)`: 
`$$p(\theta | D) =C\cdot\tilde p(\theta | D) = C\cdot Ber\big( expit( x_i ' \theta) \big)  N_{p+1}( 0, I)$$`
- **Jumping Distribution**: 
`$$Q(\theta^* | \theta) = N_{p+1}( \theta, \tau  )$$`
&lt;/small&gt;
&lt;center&gt;
&lt;img src='figures/MHalgo.png' width=30% height=50%&gt;
&lt;/center&gt;

---
## MH for Univariate Logit Model


```r
# target log density 
p_tilde &lt;- function(y, x, theta){
  p &lt;- invlogit( x %*% theta)
  lik &lt;- sum(dbinom(y, 1, p, log = T))
  pr &lt;- sum(dnorm( theta, 0, 100, log = T))
  eval &lt;- lik + pr
  return(eval)
}

iter &lt;- 1000 # number of iterations
tau &lt;- .1 # proposal sd
theta &lt;- matrix(NA, nrow = 2, ncol = iter) # for storing draws
theta[,1] &lt;- c(0,0) # initialize

for(i in 2:iter){
  # propose theta
  theta_star &lt;- MASS::mvrnorm(1, theta[,i-1] , tau*diag(2) )
  
  # accept/reject
  prop_eval &lt;- p_tilde(y, x, theta_star)
  curr_eval &lt;- p_tilde(y, x, theta[,i-1, drop=F])
  ratio &lt;- exp( prop_eval - curr_eval )
  U &lt;- rbinom(n = 1, size = 1, prob = min( c(1, ratio  ) ) )
  theta[, i] &lt;- U*theta_star + (1-U)*theta[, i-1]
}
```

---
## MH for Univariate Logit Model







&lt;img src="_bayes_computing_files/figure-html/sampleani-.gif" style="display: block; margin: auto;" /&gt;

---
## Extensions

- Proposal distributions for constrained variables. 
  - Using non-Gaussian proposals or `\(log()\)` transform.
- Sensitivity to proposal variance `\(\tau\)`.
  - **Adaptive Metropolis-Hastings**.
  - Tunes `\(\tau\)` periodically to target a desired acceptance rate.
- MH often fails in high-dimensions.
  - **Hamiltonian Monte Carlo**: leverages the gradient of `\(\tilde p\)`.

--
- Other MCMC algorithms (all similar to MH):
  - **Reversible Jump MCMC** for model selection.
  - **Split-Merge MCMC** for clustering analysis.
  - **Data Augmentation** for missing data problems. 
  

---
## Monte Carlo Integration
- We covered methods for obtaining draws `\(\{ \theta^{(t)} \}_{1:T}\)` from `\(p(\theta|D)\)`
  - Often we need summary quantities:
  `$$E[\theta | D] = \int_\Theta \theta p(\theta|D) d\theta$$`
  `$$V[\theta | D] = \int_\Theta (\theta - E[\theta | D] )^2 p(\theta|D) d\theta$$`
  `$$E[\tilde y | \tilde x, D]  =\int_\Theta E[\tilde y| \tilde x, \theta] p(\theta | D) d\theta$$`

  - Computing useful quantities requires integration - hard if `\(dim(\theta)\)` is big.

---
## Monte Carlo Integration
Recall Monte Carlo (MC) integration. Given `\(i.i.d\)` samples `\(\{\theta^{(t)}\}_{1:T} \sim p(\theta |D)\)`,
`$$E[ g(\theta) | D] = \int_\Theta g(\theta) p(\theta|D) d\theta \approx \frac{1}{T} \sum_{t=1}^T g(\theta^{(t)})$$`
--

- For posterior expectation: `\(g(\theta^{(t)}) = 1\)`

--
  
- For posterior variance: `\(g(\theta^{(t)}) = (\theta^{(t)} - \frac{1}{T}\sum_t \theta^{(t)} )^2\)`

--

- For posterior prediction: `\(g(\theta^{(t)}) =  E[\tilde y| \tilde x, \theta^{(t)}]\)`

--

Some properties:
- Convergence rate, `\(\sqrt{T}\)`, independent of `\(dim(\theta)\)`.
- Converges to posterior mean exactly based on LLN.
- We have `\(\{\theta^{(t)}\}_{1:T}\)` from MCMC, but they are not exactly `\(i.i.d\)`.
  - Effective number of draws is less than `\(T\)`.

---
## Variable Transformation 

Earlier we used MH to get `\(\{\theta_1^{(t)}\}_{1:T} \sim p(\theta|D)\)` from model 

`$$y_i \sim  Ber\Big( p_i=expit(\theta_0 + \theta_1x_{i} )\Big)$$`

--

Suppose we want estimate `\(\hat{OR}=E[exp(\theta)|D]\)`

--

`$$E[exp(\theta)|D] = \int_\Theta e^{\theta} \ p(\theta|D) d\theta \approx \frac{1}{T}\sum_{t=1}^T exp\big(\theta^{(t)} \big)$$`
--

&lt;img src="_bayes_computing_files/figure-html/exptheta-.gif" style="display: block; margin: auto;" /&gt;

---
## Bayesian Prediction
Suppose we sample parameters of this model with some prior

`$$y_i | x_i, \theta_0, \theta_1, \phi \sim N( \ \theta_0 + \theta_1x_i, \ \phi)$$`

And get posterior draws `\(\{\theta_0^{(t)}, \theta_1^{(t)}, \phi^{(t)}\}_{1:T}\)`. Given new `\(\tilde x_i\)`, form 
out-of-sample prediction

--

`$$\begin{align}
	E[\tilde y| \tilde x, D] &amp; = \int_\Theta E[\ \tilde y \ | \ \tilde x, \theta_0,\theta_1,\phi] \cdot p(\theta_0,\theta_1,\phi| D)  \\
	&amp; \approx \frac{1}{T} \sum_{i=1}^T E[\ \tilde y \ | \ \tilde x, \theta_0^{(t)},\theta_1^{(t)},\phi^{(t)}] = \frac{1}{T} \sum_{i=1}^T \theta_0^{(t)} + \theta_1^{(t)} \tilde x_i 
\end{align}$$`

--

&lt;img src="_bayes_computing_files/figure-html/bayespred-.gif" style="display: block; margin: auto;" /&gt;


---
## Approximation Methods

Upside of MCMC
  - **Asymptotically exacty**: chains guaranteed to converge to exact posterior. 
  - **Can bound errors**: easy to measure autocorrelation in chains and error in integration.

--
  
Downside of MCMC:
  - **Slow**: in complicated samplers, may take an hour to get a single draw!
  
--

Motivates the need for approximation methods: find some `\(q(\theta)\)` such that
`$$q(\theta) \approx p(\theta|D)$$`

---
## Variational Bayes 

Find approximation `\(q^*(\theta)\)` to `\(p(\theta|D)\)` such that Kullbackâ€“Leibler divergence is minimized:

$$
`\begin{align}
q* &amp; = \underset{q}{\operatorname{argmin}} KL( q || p ) \\ 
   &amp; = \underset{q}{\operatorname{argmin}} - \int_\Theta q(\theta) \ log\Big[ \frac{p(\theta|D)}{q(\theta)}\Big]d\theta
\end{align}`
$$
This is too hard of a search problem - space of `\(q(\theta)\)` is too large.


--

Restrict `\(q(\theta)\)` to the "mean-field family",
$$ q(\theta) = \prod_{i=1}^p q_i(\theta_i) $$

---
## Mean-Field Variational Bayes

Then the solution for each `\(q_j(\theta_j)\)` is 
`$$\log q_j^*(\theta_j) \propto E_{ \ \theta_{-j}\sim q_{-j}(\theta_{-j} )}\Big[ \log p(D|\theta)p(\theta) \Big]$$`
These updating equations define a **Coordinate Ascent** algorithm:

- **Initialize**: `\(q_j(\theta_j)^{(0)}=q_j^{(0)}\)` for `\(j=1, \dots, p\)`.
- **Update**: for `\(t=1, \dots, T\)`.
  - `\(q_1^{(t)}\)` conditional on `\(q_2^{(t-1)}, \dots, q_p^{(t-1)}\)`.
  - `\(q_2^{(t)}\)` conditional on `\(q_1^{(t)}, q_3^{(t-1)}, \dots, q_p^{(t-1)}\)`
  - `\(q_3^{(t)}\)` conditional on `\(q_1^{(t)}, q_2^{(t)}, q_4^{(t-1)}, \dots, q_p^{(t-1)}\)`
  - ...
  - `\(q_p^{(t)}\)` conditional on `\(q_1^{(t)}, q_2^{(t)}, q_3^{(t)}, \dots, q_p-1^{(t-1)}\)`

---
## Regression with Variational Bayes
Consider model for `\(D = (y_i, x_{0i}, x_{1i})_{1:n}\)` with known `\(\phi\)`

`$$y_i | \theta \sim N\big(x_{0i}\theta_0 + x_{1i}\theta_1, \phi\big)$$`
With prior `\(p(\theta_0) p(\theta_1) = N(0, \tau)N(0, \tau)\)`,

$$\log p(D|\theta)p(\theta) \propto \log \Big[ \prod_{n=1}^N N(y_i; x_0\theta_0 + x_1\theta_1, \phi )\Big] + \log N(\theta_0; 0, \tau )  + \log N(\theta_1; 0, \tau ) $$
---
## Regression with Variational Bayes

We assume mean-field family `\(q(\theta) = q_0(\theta_0)q_1(\theta_1)\)`. 

Using the solution expression for `\(\log q^*_j\)`, 

$$
`\begin{align}
\log q_0^*(\theta_0) &amp; \propto E_{ \ \theta_1\sim q_1}\Big[ \log p(D|\theta)p(\theta) \Big] \\
                    &amp; \propto - \frac{1}{2(\frac{\phi}{\sum_i x_{0i}^2 + \frac{\phi}{\tau} })} \Big\{ \theta_0^2 - 2\theta_0\Big[ \frac{\sum_ix_{0i}y_i - E_{\theta_1\sim q_1}[\theta_1]\sum_i x_{0i}x_{1i} }{ \sum_i x_{0i}^2 + \frac{\phi}{\tau}} \  \Big] \Big\} \\
                    \Rightarrow q_0^*(\theta_0) &amp; \stackrel{d}{=} N(\mu_0, \lambda_0) 
\end{align}`
$$

Where `\(\mu_0 = \frac{\sum_ix_{0i}y_i - E_{\theta_1\sim q_1}[\theta_1]\sum_i x_{0i}x_{1i} }{ \sum_i x_{0i}^2 - \frac{\tau}{2}}\)` and `\(\lambda_0 = \frac{\phi}{\sum_i x_{0i}^2 - \frac{\tau}{2} }\)`.

- Note dependence on `\(E_{\theta_1\sim q_1}[\theta_1]\)`
- Math is same for `\(q_1^*(\theta_1)\stackrel{d}{=}N(\mu_1, \lambda_1)\)`, with 
  - `\(\mu_1 = \frac{\sum_ix_{1i}y_i - E_{\theta_0\sim q_0}[\theta_0]\sum_i x_{0i}x_{1i} }{ \sum_i x_{1i}^2 + \frac{\phi}{\tau}}\)` and `\(\lambda_1 = \frac{\phi}{\sum_i x_{1i}^2 + \frac{\phi}{\tau} }\)`



---
## Coordinate Ascent for VB

Note `\(\lambda_1, \lambda_0\)` are known. They don't need to be updated.

Coordinate ascent with updating equations:

- Initialize `\(\mu_0^{(0)}, \mu_1^{(0)}\)`
- for `\(t=1, \dots, T\)`,
  - `\(\mu_0^{(t)} = \frac{\sum_ix_{0i}y_i - \mu_1^{(t-1)}\sum_i x_{0i}x_{1i} }{ \sum_i x_{0i}^2 + \frac{\phi}{\tau}}\)`
  - `\(\mu_1^{(t)} = \frac{\sum_ix_{1i}y_i - \mu_0^{(t)}\sum_i x_{0i}x_{1i} }{ \sum_i x_{1i}^2 + \frac{\phi}{\tau}}\)`

`$$p(\theta|D) \approx q(\theta_0) q(\theta_1) = N\Big\{ \begin{bmatrix} \mu_0^{(T)} \\ \mu_1^{(T)} \\\end{bmatrix} , 
                      \begin{bmatrix} \lambda_0 &amp; 0\\ 0 &amp; \lambda_1 \\ \end{bmatrix} \Big\}$$`


---
## Coordinate Ascent for VB




&lt;img src="_bayes_computing_files/figure-html/vb_plotting-.gif" style="display: block; margin: auto;" /&gt;

---
&lt;img src="_bayes_computing_files/figure-html/vb_v_mcmc-1.png" style="display: block; margin: auto;" /&gt;


---
## Summary

- MCMC Sampling 
  - Gibbs Sampling
  - Metropolis-Hastings 
- Monte Carlo Integration
  - Computing posterior summaries
- Posterior Approximation with VB
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="style/libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
