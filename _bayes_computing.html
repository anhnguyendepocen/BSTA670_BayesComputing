<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>BSTA 670:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Arman Oganisian   aoganisi@upenn.edu" />
    <meta name="date" content="2019-04-23" />
    <link rel="stylesheet" href="style/penn.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# BSTA 670:
## Bayesian Computation: Sampling, Integration, and Approximate Methods
### Arman Oganisian <br/> <a href="mailto:aoganisi@upenn.edu">aoganisi@upenn.edu</a>
### April 23, 2019

---






## Overview of Bayesian Inference
- Parameter vector `\(\theta \in \mathbb{R}^p\)` and data `\(D\)`.
- `\(\mathcal{L}(  \theta |  D) = p( D|  \theta)\)` with prior `\(p(\theta)\)` over parameters space.

$$
`\begin{align}
	p(\theta | D) &amp; = C\cdot p( D|  \theta) p( \theta) \\
					&amp; \propto p( D|  \theta) p( \theta) \\
\end{align}`
$$
- Inference engines:
  - Frequentist: **optimization methods** for maximizing `\(p( D| \theta)\)`.
  - Bayesian: **sampling methods** for drawing from `\(p( \theta | D)\)`.
    - Difficulty since `\(C\)` unknown
    - 

---
## Gibbs Sampler for Linear Regression

- Data `\(D = (y_i,  x_i)_{1:n}\)` and `\(\theta = (\beta, \phi)\)`, where `\(x_i, \beta \in \mathbb{R}^{p+1}\)`.
- `\(p( D |  \theta) = p(y_i |  x_i,  \theta ) \stackrel{d}{=} N( x_i '  \beta, \phi)\)`.
- If we use joint prior `\(p(\theta) = p(\beta)p(\phi) = N_{p+1}( 0,  I)IG(\alpha, \lambda)\)`, then

--

  - `\(p(\beta | \phi, D) = N_{p+1}\Big( (I + \frac{1}{\phi} X' X )^{-1}( \frac{1}{\phi} X' y)  , (I + \frac{1}{\phi} X' X )^{-1} \Big)\)`.
  - `\(p(\phi | \beta, D) = IG(\alpha + n/2, \lambda + \frac{1}{2} ( y -  X \beta)'( y - X \beta) )\)`
	
--

- **Gibbs Sampling**: sample from these two conditionals in alternating fashion
  -  `\(\beta^{(t)} | \phi^{(t-1)} \sim N_{p+1} \Big( ( I + \frac{1}{ \phi^{(t-1)}}  X'  X )^{-1}( \frac{1}{  \phi^{(t-1)} }  X'  y)  , ( I + \frac{1}{ \phi^{(t-1)}}  X'  X )^{-1} \Big)\)`.
  - `\(\phi^{(t)} |  \beta^{(t)} \sim IG(\alpha + n/2, \lambda + \frac{1}{2} ( y -  X  \beta^{(t)})'( y -  X  \beta^{(t)} ) )\)`.
- Claim: The samples `\(\{\beta^{(t)}, \phi^{(t)} \}_{1:T}\)` converge to draws from the posterior `\(p(\beta, \phi |  D)\)`.


---
## Gibbs Sampling



```r
for( i in 2:iter){
  post_cov &lt;- solve(Imat + (1/phi[i-1]) * xtx)
  post_mean &lt;- post_cov %*% ((1/phi[i-1]) * t(X) %*% y)
  beta[, i]  &lt;- MASS::mvrnorm(1, post_mean , post_cov )
  
  post_rate &lt;- 100 + .5*sum((y - X %*% beta[, i, drop=F])^2)
  post_shape &lt;- 5 + n/2
  phi[i] &lt;- invgamma::rinvgamma(1, post_shape, rate = post_rate)
}
```

--

- We can plot the sequences or "chains":  `\(\{\beta^{(t)} \}_{1:T}\)` and `\(\{\phi^{(t)} \}_{1:T}\)`.
- These are the **Monte Carlo Markov Chains**.
	- **Monte Carlo**: each element of the chain is randomly drawn/simulated.
	- **Markov**: `\(\theta^{(t)}\)` only depends on the previous element `\(\theta^{(t-1)}\)`.
- Markov Chains require an explicit **transition distribution/rule**, `\(T(\beta^{(t+1)} | \beta^{(t)})\)`.

---
## Gibbs Sampling

&lt;img src="_bayes_computing_files/figure-html/gibbs_anim-.gif" style="display: block; margin: auto;" /&gt;

---
## MCMC - Checks and Limitations of Gibbs
- After sampling, must conduct visual and formal checks for
	- Convergence.
	- Autocorrelation. 
	- Sensitivity to initial values.
- Gibbs requires known conditional posteriors: `\(p(\beta | \phi, D)\)`, `\(p(\phi | \beta, D)\)`.
- In models without conjugacy, these are unknown - all we know is the form of `\(p(\theta | D)\)` up to a proportionality constant.

---
## Sampling for a Logistic Regression

- Data `\(D = (y_i, x_i)_{1:n}\)`, where `\(x_i \in \mathbb{R}^{p+1}\)` and `\(y_i \in \{0,1\}\)`.	
`$$p( D | \theta) \stackrel{d}{=} Ber\big( expit( x_i ' \theta) \big)$$` 

`$$p(\theta) \stackrel{d}{=} N_{p+1}( 0, I)$$`

- Posterior is unknown:
`$$p(\theta | D) \propto Ber\big( expit( x_i ' \theta) \big) N_{p+1}( 0, I)$$`
- Gibbs can't be used here.

---
## The Metropolis-Hastings Sampler
&lt;small&gt;
Along with initial value, `\(\theta^{(0)}\)`, MH algorithm requires three inputs:
- **Unnormalized target density**, `\(\tilde p(\theta | D)\)`: 
`$$p(\theta | D) =C\cdot\tilde p(\theta | D) = C\cdot Ber\big( expit( x_i ' \theta) \big)  N_{p+1}( 0, I)$$`
- **Jumping Distribution**: 
`$$Q(\theta^* | \theta) = N_{p+1}( \theta, \tau  )$$`
&lt;/small&gt;
&lt;center&gt;
&lt;img src='figures/MHalgo.png' width=80%&gt;
&lt;/center&gt;

---
## MH for Univariate Logit Model


```r
# target log density 
p_tilde &lt;- function(y, x, theta){
  p &lt;- invlogit( x %*% theta)
  lik &lt;- sum(dbinom(y, 1, p, log = T))
  pr &lt;- sum(dnorm( theta, 0, 100, log = T))
  eval &lt;- lik + pr
  return(eval)
}

iter &lt;- 1000 # number of iterations
tau &lt;- .1 # proposal sd
theta &lt;- matrix(NA, nrow = 2, ncol = iter) # for storing draws
theta[,1] &lt;- c(0,0) # initialize

for(i in 2:iter){
  # propose theta
  theta_star &lt;- MASS::mvrnorm(1, theta[,i-1] , .001*diag(2) )
  
  # accept/reject
  prop_eval &lt;- p_tilde(y, x, theta_star)
  curr_eval &lt;- p_tilde(y, x, theta[,i-1, drop=F])
  ratio &lt;- exp( prop_eval - curr_eval )
  U &lt;- rbinom(n = 1, size = 1, prob = min( c(1, ratio  ) ) )
  theta[, i] &lt;- U*theta_star + (1-U)*theta[, i-1]
}
```

---
## MH for Univariate Logit Model







&lt;img src="_bayes_computing_files/figure-html/sampleani-.gif" style="display: block; margin: auto;" /&gt;

---
## Why does MH Work?
- Need to show that `\(p(\theta|D)\)` is the **equilibrium** or **stationary distribution** of `\(\{\theta^{(t)} \}_{t=1}^\infty\)`.
- This is gauranteed if `\(p(\theta|D)\)` satisfies **detailed balance**. For any two values `\(\theta^i, \theta^j\)`,
  - `\(p(\theta^i|D) T(\theta^j|\theta^i) = p(\theta^j|D) T(\theta^i | \theta^j)\)`
  - For MH, with symmetric proposal, `\(T(\theta^j|\theta^i) = Q(\theta^{j}| \theta^{i}) \min(1, \frac{p(\theta^j|D)Q(\theta^j|\theta^i)}{p(\theta^i|D) Q(\theta^i|\theta^j)} )\)`
  - Without loss of generality, let `\(\frac{p(\theta^j|D)}{p(\theta^i|D)} &lt; 1\)`
  
$$
`\begin{align}
  p(\theta^i|D) T(\theta^j|\theta^i) &amp; = p(\theta^i|D) \cdot Q(\theta^{j}| \theta^{i}) \cdot \frac{p(\theta^j|D)Q(\theta^j|\theta^i)}{p(\theta^i|D) Q(\theta^i|\theta^j)} \\
  &amp; = Q(\theta^{j}| \theta^{i}) \cdot p(\theta^j|D) \\
\end{align}`
$$

---
## Extensions

- Proposal distributions for constrained variables. 
  - Using non-Gaussian proposals or `\(log()\)` transform.
- Sensitivity to proposal variance `\(\tau\)`.
  - **Adaptive Metropolis-Hastings**.
  - Tunes `\(\tau\)` periodically to target a desired acceptance rate.
- MH often fails in high-dimensions.
  - **Hamiltonian Monte Carlo**: leverages the gradient of `\(\tilde p\)`.

--
- Other MCMC algorithms (all similar to MH):
  - **Reversible Jump MCMC** for model selection.
  - **Split-Merge MCMC** for clustering analysis.
  - **Data Augmentation** for missing data problems. 
  

---
## Monte Carlo Integration
- We covered methods for obtaining draws `\(\{ \theta^{(t)} \}_{1:T}\)` from `\(p(\theta|D)\)`
  - Often we need summary quantities:
  `$$E[\theta | D] = \int_\Theta \theta p(\theta|D) d\theta$$`
  `$$V[\theta | D] = \int_\Theta (\theta - E[\theta | D] )^2 p(\theta|D) d\theta$$`
  `$$E[\tilde y | \tilde x, D]  =\int_\Theta E[\tilde y| \tilde x, \theta] p(\theta | D) d\theta$$`

  - Computing useful quantities requires integration - hard if `\(dim(\theta)\)` is big.

---
## Monte Carlo Integration
Recall Monte Carlo (MC) integration. Given `\(i.i.d\)` samples `\(\{\theta^{(t)}\}_{1:T} \sim p(\theta |D)\)`,
`$$E[ g(\theta) | D] = \int_\Theta g(\theta) p(\theta|D) d\theta \approx \frac{1}{T} \sum_{t=1}^T g(\theta^{(t)})$$`
--

- For posterior expectation: `\(g(\theta^{(t)}) = 1\)`

--
  
- For posterior variance: `\(g(\theta^{(t)}) = (\theta^{(t)} - \frac{1}{T}\sum_t \theta^{(t)} )^2\)`

--

- For posterior prediction: `\(g(\theta^{(t)}) =  E[\tilde y| \tilde x, \theta^{(t)}]\)`

--

Some properties:
- Convergence rate ($\sqrt{T}$) independent of `\(dim(\theta)\)`.
- Converges to posterior mean exactly based on LLN.
- We have `\(\{\theta^{(t)}\}_{1:T}\)` from MCMC, but they are not exactly `\(i.i.d\)`.
  - Effective number of draws is less than `\(T\)`.

---
## Variable Transformation 

Earlier we used MH to get `\(\{\theta_1^{(t)}\}_{1:T} \sim p(\theta|D)\)` from model 

`$$y_i \sim  Ber\Big( p_i=expit(\theta_0 + \theta_1x_{i} )\Big)$$`

--

Suppose we want estimate `\(\hat{OR}=E[exp(\theta)|D]\)`

--

`$$E[exp(\theta)|D] = \int_\Theta e^{\theta} \ p(\theta|D) d\theta \approx \frac{1}{T}\sum_{t=1}^T exp\big(\theta^{(t)} \big)$$`
--

&lt;img src="_bayes_computing_files/figure-html/exptheta-.gif" style="display: block; margin: auto;" /&gt;

---
## Bayesian Prediction



---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="style/libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
