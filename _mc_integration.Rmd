---
title: "BSTA 670:"
subtitle: "Bayesian Computation: Sampling, Integration, and Approximate Methods"
author: "Arman Oganisian <br/> aoganisi@upenn.edu"
date: "April 23, 2019"
output:
  xaringan::moon_reader:
    chakra: style/libs/remark-latest.min.js
    lib_dir: style/libs
    css: style/penn.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

library(RefManageR)
library(kableExtra)
library(dplyr)

#BibOptions(check.entries = FALSE, bib.style = "numeric", style = "markdown",
#           dashed = TRUE)
#file.name <- system.file("Bib", "biblatexExamples.bib", package = "RefManageR")
#bib <- ReadBib("bibliography.bib")
```

```{r, echo=FALSE, warning=FALSE, include=FALSE}
inline <- function(x = "") paste0("`` `r ", x, "` ``")

knitr::opts_chunk$set(echo = TRUE, comment = "", fig.height = 5.5, fig.width = 5.5, cache = F)
```

## Overview of Bayesian Inference
- Parameter vector $\theta \in \mathbb{R}^p$ and data $D$.
- $\mathcal{L}(  \theta |  D) = p( D|  \theta)$ with prior $p(\theta)$ over parameters space.

$$
\begin{align}
	p(\theta | D) & = C\cdot p( D|  \theta) p( \theta) \\
					& \propto p( D|  \theta) p( \theta) \\
\end{align}
$$
- Inference engines:
  - Frequentist: **optimization methods** for maximizing $p( D| \theta)$.
  - Bayesian: **sampling methods** for drawing from $p( \theta | D)$.
    - Difficulty: $C$ unknown, $p(\theta|D)$ may be unknown.

---
## Gibbs Sampler for Linear Regression

- Data $D = (y_i,  x_i)_{1:n}$ and $\theta = (\beta, \phi)$, where $x_i, \beta \in \mathbb{R}^{p+1}$.
- $p( D |  \theta) = p(y_i |  x_i,  \theta ) \stackrel{d}{=} N( x_i '  \beta, \phi)$.
- If we use joint prior $p(\theta) = p(\beta)p(\phi) = N_{p+1}( 0,  I)IG(\alpha, \lambda)$, then

--

  - $p(\beta | \phi, D) = N_{p+1}\Big( (I + \frac{1}{\phi} X' X )^{-1}( \frac{1}{\phi} X' y)  , (I + \frac{1}{\phi} X' X )^{-1} \Big)$.
  - $p(\phi | \beta, D) = IG(\alpha + n/2, \lambda + \frac{1}{2} ( y -  X \beta)'( y - X \beta) )$
	
--

- **Gibbs Sampling**: sample from these two conditionals in alternating fashion
  -  $\beta^{(t)} | \phi^{(t-1)} \sim N_{p+1} \Big( ( I + \frac{1}{ \phi^{(t-1)}}  X'  X )^{-1}( \frac{1}{  \phi^{(t-1)} }  X'  y)  , ( I + \frac{1}{ \phi^{(t-1)}}  X'  X )^{-1} \Big)$.
  - $\phi^{(t)} |  \beta^{(t)} \sim IG(\alpha + n/2, \lambda + \frac{1}{2} ( y -  X  \beta^{(t)})'( y -  X  \beta^{(t)} ) )$.
- Claim: The samples $\{\beta^{(t)}, \phi^{(t)} \}_{1:T}$ converge to draws from the posterior $p(\beta, \phi |  D)$.


---
## Gibbs Sampling
```{r, echo=F, warning=FALSE, message=FALSE, error=FALSE}
set.seed(111)
## simulate data
n <- 10000
X <- cbind(1, rnorm(n))
y <- X %*% matrix(c(10,10), ncol=1) + rnorm(n,0, 10)
iter <- 1000

## create shells to store Gibbs draws
phi <- numeric(length = iter)
beta <- matrix(NA, ncol=iter, nrow = ncol(X) )

## initialize
phi[1] <- 90

## pre-compute some quantities
xtx <- t(X) %*% X
xty  <- t(X) %*% y
Imat <- diag(ncol(X))
```

```{r, echo=T, warning=FALSE, message=FALSE, error=FALSE}
for( i in 2:iter){
  post_cov <- solve(Imat + (1/phi[i-1]) * xtx)
  post_mean <- post_cov %*% ((1/phi[i-1]) * t(X) %*% y)
  beta[, i]  <- MASS::mvrnorm(1, post_mean , post_cov )
  
  post_rate <- 100 + .5*sum((y - X %*% beta[, i, drop=F])^2)
  post_shape <- 5 + n/2
  phi[i] <- invgamma::rinvgamma(1, post_shape, rate = post_rate)
}
```

--

- We can plot the sequences or "chains":  $\{\beta^{(t)} \}_{1:T}$ and $\{\phi^{(t)} \}_{1:T}$.
- These are the **Monte Carlo Markov Chains**.
	- **Monte Carlo**: each element of the chain is randomly drawn/simulated.
	- **Markov**: $\theta^{(t)}$ only depends on the previous element $\theta^{(t-1)}$.
- Markov Chains require an explicit **transition distribution/rule**, $T(\beta^{(t+1)} | \beta^{(t)})$.

---
## Gibbs Sampling

```{r gibbs_anim, echo=F, warning=FALSE, message=FALSE, error=FALSE, cache=T, fig.show='animate', fig.width=10, interval=.25, fig.height=6, cache=FALSE, aniopts="loop", animation.hook='gifski', fig.align='center', echo=F, eval=T}

par(mfrow=c(2,3))
for( i in seq(10, 1000, 10)){
  plot(phi[1:i], type='l', xlim=c(2,1000), xlab='Iteration, t' )
  abline(h=10^2, col='red', lwd=2)
  
  plot(beta[1,2:i], type='l', xlim=c(2,1000), xlab='Iteration, t')
  abline(h=10, col='red', lwd=2)
  
  plot(beta[2,2:i], type='l', xlim=c(2,1000), xlab='Iteration, t')
  abline(h=10, col='red', lwd=2)
  
  hist(phi[2:i], xlab='Posterior Draw' )
  abline(v=10^2, col='red', lwd=2)
  
  hist(beta[1,2:i], xlab='Posterior Draw')
  abline(v=10, col='red', lwd=2)
  
  hist(beta[2,2:i], xlab='Posterior Draw')
  abline(v=10, col='red', lwd=2)
}

```

---
## MCMC - Checks and Limitations of Gibbs
- After sampling, must conduct visual and formal checks for
	- Convergence.
	- Autocorrelation. 
	- Sensitivity to initial values.
- Gibbs requires known conditional posteriors: $p(\beta | \phi, D)$, $p(\phi | \beta, D)$.
- In models without conjugacy, these are unknown - all we know is the form of $p(\theta | D)$ up to a proportionality constant.

---
## Sampling for a Logistic Regression

- Data $D = (y_i, x_i)_{1:n}$, where $x_i \in \mathbb{R}^{p+1}$ and $y_i \in \{0,1\}$.	
$$p( D | \theta) \stackrel{d}{=} Ber\big( expit( x_i ' \theta) \big)$$ 

$$p(\theta) \stackrel{d}{=} N_{p+1}( 0, I)$$

- Posterior is unknown:
$$p(\theta | D) \propto Ber\big( expit( x_i ' \theta) \big) N_{p+1}( 0, I)$$
- Gibbs can't be used here.

---
## The Metropolis-Hastings Sampler
<small>
Along with initial value, $\theta^{(0)}$, MH algorithm requires three inputs:
- **Unnormalized target density**, $\tilde p(\theta | D)$: 
$$p(\theta | D) =C\cdot\tilde p(\theta | D) = C\cdot Ber\big( expit( x_i ' \theta) \big)  N_{p+1}( 0, I)$$
- **Jumping Distribution**: 
$$Q(\theta^* | \theta) = N_{p+1}( \theta, \tau  )$$
</small>
<center>
<img src='figures/MHalgo.png' width=80%>
</center>

---
## MH for Univariate Logit Model

```{r, echo=T, eval=F}
# target log density 
p_tilde <- function(y, x, theta){
  p <- invlogit( x %*% theta)
  lik <- sum(dbinom(y, 1, p, log = T))
  pr <- sum(dnorm( theta, 0, 100, log = T))
  eval <- lik + pr
  return(eval)
}

iter <- 1000 # number of iterations
tau <- .1 # proposal sd
theta <- matrix(NA, nrow = 2, ncol = iter) # for storing draws
theta[,1] <- c(0,0) # initialize

for(i in 2:iter){
  # propose theta
  theta_star <- MASS::mvrnorm(1, theta[,i-1] , .001*diag(2) )
  
  # accept/reject
  prop_eval <- p_tilde(y, x, theta_star)
  curr_eval <- p_tilde(y, x, theta[,i-1, drop=F])
  ratio <- exp( prop_eval - curr_eval )
  U <- rbinom(n = 1, size = 1, prob = min( c(1, ratio  ) ) )
  theta[, i] <- U*theta_star + (1-U)*theta[, i-1]
}


```

---
## MH for Univariate Logit Model

```{r, echo=F}
library(LaplacesDemon)

x <- cbind(1, rnorm(1000))
y <- rbinom(1000, 1, invlogit( 1 - 1*x[,2] ) )

p_tilde <- function(y, x, theta){
  p <- invlogit( x %*% theta)
  res <- sum(dbinom(y, 1, p, log = T)) + sum(dnorm( theta, 0, 100, log = T))
  return(res)
}

iter <- 1000
tau <- .1

theta <- matrix(NA, nrow = 2, ncol = iter)
theta[,1] <- c(0,0)

theta2 <- matrix(NA, nrow = 2, ncol = iter)
theta2[,1] <- c(0,0)

theta3 <- matrix(NA, nrow = 2, ncol = iter)
theta3[,1] <- c(0,0)

```

```{r, echo=F}
library(LaplacesDemon)
par(mfrow=c(1,2))
for(i in 2:iter){
  
  theta_star <- MASS::mvrnorm(1, theta[,i-1] , .001*diag(2) )
  ratio <- exp(p_tilde(y, x, theta_star) - p_tilde(y, x, theta[,i-1, drop=F]) )
  U <- rbinom(n = 1, size = 1, prob = min( c(1, ratio  ) ) )
  theta[, i] <- U*theta_star + (1-U)*theta[, i-1]

  theta_star <- MASS::mvrnorm(1, theta2[,i-1] , .1*diag(2) )
  ratio <- exp(p_tilde(y, x, theta_star) - p_tilde(y, x, theta2[,i-1, drop=F]) )
  U <- rbinom(n = 1, size = 1, prob = min( c(1, ratio  ) ) )
  theta2[, i] <- U*theta_star + (1-U)*theta2[, i-1]
  
  theta_star <- MASS::mvrnorm(1, theta3[,i-1] , 1*diag(2) )
  ratio <- exp(p_tilde(y, x, theta_star) - p_tilde(y, x, theta3[,i-1, drop=F]) )
  U <- rbinom(n = 1, size = 1, prob = min( c(1, ratio  ) ) )
  theta3[, i] <- U*theta_star + (1-U)*theta3[, i-1]

}
```

```{r, echo=F}
z <- matrix(nrow=100, ncol=100)
mu = rowMeans(theta)
sigma = cov( t(theta))

x.points <- quantile(theta[1,],probs=seq(.01,1,.01))
y.points <- quantile(theta[2,],probs=seq(.01,1,.01))

for (i in 1:100) {
  for (j in 1:100) {
    z[i,j] <- mvtnorm::dmvnorm(c(x.points[i],y.points[j]), mean=mu,sigma=sigma)
  }
}

```

```{r sampleani, cache=T, fig.show='animate', fig.width=10,interval=.25, fig.height=7.5, cache=FALSE, aniopts="loop", animation.hook='gifski', fig.align='center', echo=F, eval=T}

for(i in seq(10, 1000, 10)){
  par(mfrow=c(2,3))
  
  plot(theta[1,2:i], theta[2,2:i], type='l', xlim=c(-.25,1.5), ylim=c(-1.5,.5),
       xlab='theta_0',ylab='theta_1',
       main='Proposal Variance = .001')
  contour(x.points,y.points,z, col='red',add=T,lwd = 1)
  lines(theta[1,2:i], theta[2,2:i])
  
  plot(theta2[1,2:i], theta2[2,2:i], type='l', xlim=c(-.25,1.5), ylim=c(-1.5,.5),
       xlab='theta_0',ylab='theta_1',
       main='Proposal Variance = .1')
  contour(x.points,y.points,z, col='red',add=T,lwd = 1)
  lines(theta2[1,2:i], theta2[2,2:i])
  
  plot(theta3[1,2:i], theta3[2,2:i], type='l', xlim=c(-.25,2), ylim=c(-2,.5),
       xlab='theta_0',ylab='theta_1',
       main='Proposal Variance = 1')
  contour(x.points,y.points,z, col='red',add=T,lwd = 1)
  lines(theta3[1,2:i], theta3[2,2:i])
  
  plot(theta[2,2:i], type='l', xlim=c(0,iter), xlab='iteration, t',
       ylab='theta_1')
  abline(h=-1, col='red', lwd=3)
  plot(theta2[2,2:i], type='l', xlim=c(0,iter), xlab='iteration, t',
       ylab='theta_1')
  abline(h=-1, col='red', lwd=3)
  plot(theta3[2,2:i], type='l', xlim=c(0,iter), xlab='iteration, t',
       ylab='theta_1')
  abline(h=-1, col='red', lwd=3)
}

```

---
## Why does MH Work?
- Need to show that $p(\theta|D)$ is the **equilibrium** or **stationary distribution** of $\{\theta^{(t)} \}_{t=1}^\infty$.
- This is gauranteed if $p(\theta|D)$ satisfies **detailed balance**. For any two values $\theta^i, \theta^j$,
  - $p(\theta^i|D) T(\theta^j|\theta^i) = p(\theta^j|D) T(\theta^i | \theta^j)$
  - For MH, with symmetric proposal, $T(\theta^j|\theta^i) = Q(\theta^{j}| \theta^{i}) \min(1, \frac{p(\theta^j|D)Q(\theta^j|\theta^i)}{p(\theta^i|D) Q(\theta^i|\theta^j)} )$
  - Without loss of generality, let $\frac{p(\theta^j|D)}{p(\theta^i|D)} < 1$
  
$$
\begin{align}
  p(\theta^i|D) T(\theta^j|\theta^i) & = p(\theta^i|D) \cdot Q(\theta^{j}| \theta^{i}) \cdot \frac{p(\theta^j|D)Q(\theta^j|\theta^i)}{p(\theta^i|D) Q(\theta^i|\theta^j)} \\
  & = Q(\theta^{j}| \theta^{i}) \cdot p(\theta^j|D) \\
\end{align}
$$

---
## Extensions

- Proposal distributions for constrained variables. 
  - Using non-Gaussian proposals or $log()$ transform.
- Sensitivity to proposal variance $\tau$.
  - **Adaptive Metropolis-Hastings**.
  - Tunes $\tau$ periodically to target a desired acceptance rate.
- MH often fails in high-dimensions.
  - **Hamiltonian Monte Carlo**: leverages the gradient of $\tilde p$.

---

---
## Bayesian Prediction

---
